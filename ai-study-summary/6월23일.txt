새로운 노트
2023.06.23 금 오후 1:10 ・ 99분 17초
이준택

AI 요약
00:01
스테이블 디트의 기술
- 그리고 이건 뭐냐면 지금까지 설명했던 드림부스와 스테이블 디트나 스테이블 디트의 어떤 인텐트 등 등의 모든 기술들을 여기서 볼 수가 있음
- 2 3주 뒤에 공유해 주신다고 함

05:52
네이버, 이미지 생성 기술 개발
- 인식도에서 헤지저체 인지를 활용을 해서 인식이나 분류단에도 성능 향상에 크게 기여할 것 같음
- 관련해서 같이 연구를 하면 성적이 많이 높아질 것 같음
- 네이버에서 투자가 굉장히 유심하게 있음
- 단순한 이미지 생성 텍스트 이미지 그런 이미지 생성뿐만 아니라 다른 쪽으로도 활용할 가능성은 100% 있다고 보기 때문에 네이버에서도 그렇고 현재 연구소에 있다고 말씀드릴 수 있음

07:39
디퓨전 모델의 두 가지 관점
- 디퓨전 모델이라는 거를 설명하는 유용한 두 가지 관점이 있는데 하나는 위쪽 에치에 모이든 디퓨전 프로베빌리스틱 모델이라고 제가 써놓은 관점이 있고 저게 아마 많이들 익숙하실 테지만 솔 딕스테인이라는 사람이 2015년에 처음 제안을 했고 비교적 최근에 변하다노가 다시 부활을 시킴
- 스코어 베이시트 모델은 디퓨전 프로베리스트 모델을 처음 제안됐을 때부터 베리에셔너 어프로치와 관련이 좀 있음
- 스코어 베이스트 디퓨전 모델은 독립적으로 발전이 돼오다가 2010년쯤에 디노이징 스코어 매칭이라는 게 나오고요 이거는 디퓨전 모델에서 많이 쓰는 로스 펑션임
- 조나단어 논문에 나오는데 그때부터 두 개의 브랜치가 하나로 합쳐지게 됨

12:15
뉴럴 네트워크의 문제점
- 딥뉴런넷을 학습시킬 줄 아니까 해지 충분히 플렉소보나 함수를 잘 맞추는 거를 많이 봐왔으니까 문제가 해결된 거 아니냐라고 할 수 있지만 그거랑 조금 다른 그냥 느라넷으로는 안 풀리는 문제가 있음
- 뉴럴 네트워크 인테그를 했을 때 1이 나오게 할 것인가를 풀기 위해서 많은 사람들이 고민을 했고 첫 번째 방법은 임버터블 슬로우라는 방법들인데 여기서는 어떤 인버터블한 트랜스포메이션을 파라미터라이즈 함
- 뉴론 네트워크 아키텍처와 인버터업을 해야 된다는 건데 우리가 알고 있는 유넬 이런 거에 사실 인버터블 하지는 않음
- 아키텍처의 컨스트레인이 많이 걸리게 됨

15:39
픽셀 샘플링
- 픽셀의 분포를 알려면 전에 있는 픽셀이 이미 샘플링이 된 상태여야만 픽셀을 샘플링 할 수 있음
- 픽셀을 순서대로 만들어야 되는 것은 픽셀을 만들어지지 않았을 때 눈 쪽을 모르기 때문임
- 텍스트에서는 거의 지배하고 있는 것 같고 이미지 도메인에서 기존에까지 가장 성공적이었던 모델이 팬임
- 오토 인코더도 행렬 연산해서 줄여가지고 버터블 매트릭스 연산하면 다시 원래 거로 돌아올 거 해서 한다고는 알고 있음

19:58
피세타의 확률 분포
- 데이터 포인트를 근사화하는 피세타를 찾는 게 모델이 찾는 게 목표임
- 실제의 데이터의 확률 분포랑 피 세타의 확률 분포의 kl div전스는 확률 분포 간의 거리를 나타내는 함수임
- 두 사이의 거리가 최소화되게 하는 게 목적임

22:56
에너지 베이스 모델의 정의
- 에너지 베이스 모델은 문제를 해결하는 많은 모델들 중에 하나의 클래스임
- 에너지 베이스 모델은 pdf의 정의를 만족하게 해야 함
- 에너지 펑션을 우리가 원하는 것은 데이터 포인트가 있는 지점에서 에너지가 낮고 없는 지점에서 에너지가 높은 것임
- 맥시멈 라이프리우드 트레이닝을 하려면 로고 티제타를 세타에 대해서 미분한 다음에 그래디얼트 베이스 옵티마이제이션을 하면 됨

27:24
edm 샘플링
- edm을 학습시키려면 edm에서 샘플링을 할 줄 알아야 됨
- edm을 매티몬 라이크에서 트레이닝 하는 거는 쉽지 않음
- 샘플링을 매트 인터레이션마다 해야 되니까 엄청나게 느린 거임

33:20
확률 모델의 엑스
- 인풋을 모델 피세타 모델 확률 모델의 엑스를 넣으면 로스 함수가 줄어들게끔 뭔가 돼야 됨
- mla 방식으로 수식이 나온 거고 전개를 해보니까 우변 항의 정리할 수 있는데 확률 샘플링해서 우리가 뭔가 할 수 있다 구할 수 있다로 바뀜
- 비세타의 엑스를 넣었는 거에 샘플링 했는 거를 가지고 트레이닝을 시킬 수 있음
- 조선이 수가 없어 데이터 목표라고 하고 이걸 모델 대표라고 하면 됨
- 트레이닝 하는 모델 문포랑 데이터 포 페이블레션 점점 리마 하는 거임

38:43
로스의 계산
- 로스를 구할 때 굉장히 어려우니까 미분을 해서 미분끼리 비교를 하자고 함
- 그렇게 하면 제트 관련된 텀이 빠지고 고산이 쉬워짐
- 데이터 크기가 커지면 커질수록 기하급수적으로 커지니까 스케일업을 하지 않음

42:02
노이지안 분포의 코어
- 기노의식 스코어 매칭은 데이터 디스트레이션의 스코어를 효율적으로 배우는 방법을 제시하지 못함
- 캐릭터에 노이즈를 뿌리면 노지간 디스트로션과의 스코어 매칭은 쉽게 할 수 있다는 걸 보겠음
- 노이지안 데이터 디스테이션이랑 스코어 매칭을 하는 것임
- 노이지안 분포의 코어를 배우면 노이지안 분포의 코어를 배움

45:55
라이크 후드의 개념
- 어떤 분포를 알려면 샘플링하는 방법으로 라이크 후드를 구할 수 있음
- 원본 이미지에다가 노이즈를 끼었는 게 노이즈를 더 했으니까 그 변화량이니까 그냥 개념적으로 보면 바로 노이즈라는 수식적으로 보면 이제 아까 전에 이거 노이즈 분포 이거 있었으니까 요 로그 치해서 미분하면 결국 이 식이 나온 거임

51:54
데이터 디스트레이션의 스코어
- 댄스트1션에 두 개의 미분 값을 미분 값의 차이를 줄이겠다는 게 콘셉트였던 것 같음
- 데이터 디스트레이션의 스코어를 효율적으로 배우는 방법을 제시하지 못함
- 스코어를 정하는 벡터 필드를 타고 올라가는 거랑 똑같음
- 샘플링을 할 때 스코어만 알면 샘플링을 할 수 있음

57:24
pdf의 스코어 에스티메이션
- pdf의 로벤스트 리전에서 스코어 에스티메이션이 안정확하다는 문제가 있음
- 데이터를 샘플링 할 때는 노이즈에서 시작을 할 것임
- 모델 자체는 학습할 때 로우 데스트 리전에서 이니셜 라이즈를 타고 이렇게 타고 올라가야 함

1:00:28
매니폴드 상에 없는 로이즈
- 학습 데이터에 없는 매니폴드 상에 없는 로이즈에 대해서는 스코어를 정확하게 계산을 할 수 없음
- 학습을 못하니까 학습이 잘 안 됨
- 노이즈를 넣어서 학습을 할 때는 방향이 어디로 가야 된다는 정보가 있어야 함

1:06:43
로데스트 유전의 문제
- 로데스트 유전 때에 발생하는 문제는 피 데이터의 분포를 뽑았는데 뽑고 나니까 어떤 분포? 피1x p2x에서 이렇게 뽑히게 되는데 파이에 의해서 비율 조절되게 되는데 우리가 아까 전에 쓰는 방식 자체가 한 번 1차 미군했던 그걸 가지고 우리 비교하기로 했으니까 그렇게 해버리게 되면 파이가 사라지기 때문에 문제가 생기는 것임

1:09:39
제너리티 모델링 바이 에스티메틴 브레디한 속 리타 데스트리션
- pdf의 로데스 이전 때문에 생기는 문제를 해결하는 게 바로 제너리티 모델링 바이 에스티메틴 브레디한 속 리타 데스트리션이라는 곡임
- 저자의 아이디어는 데이터에 논지를 뿌리면 로우 데스트 미션이 채워질 거라는 것임
- 저자는 무의지를 조금씩 뿌리면서 샘플링을 하면 되는 거 아니냐라는 생각을 함
- 저자는 노이즈 컨디션을 스코어 네트워크라고 하고 학습을 시킴

1:15:53
디퓨전 방법론
- 수도 코드를 보면 될 것 같은데 디퓨전인 거잖아요. 맨 처음에 션이 제일 안에 있는 드로우가 우리가 출력되는 값이니까 생성되는 거임
- 로컬 맥시마에 빠지지 않기 위해서 쓰는 노이즈라고 했던 거 샘플링 할 때 로컬 맥시마에 빠지지 않게 하는 거임
- 제트 티에 대해서 좀 더 설명하자면 로컬 맥시머에 빠지지 않게 하기 위해서 샘플링 할 때 쓰는 거임
- 디퓨전 방법론 자체가 노이즈를 끼얹다 보니까 로컬 미니마에 안 빠지게 되는 그런 장점이 있음
- 그래서 만드는 것 같음

1:24:42
스코어 네트워크 디피전 모델
- 스코베이스트 모델이랑 디퓨전 프로베 베스트 모델을 하나로 합치면 유니파이드 프레이머커에 제안을 함
- 디퓨전 모델을 디터미니스틱하게 샘플링을 하면 노말라진 슬로우가 돼서 마이클 리드 계산을 정확하게 할 수 있음
- 원투 원 맵이기 때문에 데이터가 정해지면 레이터 드렉터도 정해져서 뭔가 스모드한 인터폴레이션을 할 수도 있음
- 컨트롤러블 제노레이션이라는 건데 언컨디셔널하게만 학습한 스코어 네트워크 디피전 모델로 인테이팅도 할 수 있고 클래스 컨디션 제너레이션도 할 수 있고 슈퍼 레졸루션도 할 수 있음
- sd는 그럴 때 더 유용한 더 리얼리티한 모델인 거임

1:30:38
앤더슨의 스토캐스틱
- 앤더슨이 말하는 거는 이해는 하는데 수식이 어떤 의미인가 싶어서 물어봄
- t 마이너스 1위라는 게 나와야 설명할 수 있을 것 같음
- 스토캐스틱은 통계적으로 랜덤 노이즈가 끼어 있냐 아니냐 차이라고 함

1:33:43
오프라인 모임
- 오늘은 시간이 2시 44분인데 너무 어려우니까 오늘 배운 거 조금 정리하는 시간을 가지면 좋을 것 같음
- 우리는 오프라인으로 할 건데 그전에 관심 있으신 분은 같이 참여하면 좋을 것 같음


clovanote.naver.com