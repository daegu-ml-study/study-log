새로운 노트
2023.06.09 금 오후 1:03 ・ 72분 14초
이준택

AI 요약
00:04
엔비디아의 iclr 20에
- 디퓨전 모델은 엔비디아에서 냈던 iclr 20에라고 보시면 될 것 같고 엔비디아에서 냈던 iclr 20에를 선정함
- 디퓨전 모델은 제너레이터와 디스크리미네이터 두 개를 두고 제너레이터가 생성해낸 이미지를 디스크리미네이터가 진짜냐 가짜냐를 구분하면서 학습하는 것임
- 제너레이터를 학습하고 디스크리미네이터를 학습을 하는데 이 두 개의 로스 함수를 보면 이 두 개의 밋맥스 형태의 로스 함수로 표현됨

06:31
제너레이터의 가정
- 논문이 ddpm이랑 개이랑 합쳐서 만들어진 논문이라고 함
- 제너레이터에 별도의 제약 조건 없이 계속 출원을 생성하게 되는 가정을 함
- 가오시안 분포 가우시안 분포를 가정이 필요가 없는 건가라고 함

08:03
ddpm의 포드
- 질문자가 ddpm이 포드 하게 되면 맨 xt가 가오션 포라는 거를 잘 모르고 질문한 것 같음
- 학습 방법은 적으로 이해를 하면 최종적으로 xt를 봤을 때 로이스트랑 비슷할 것 같음
- 디스크리에이터가 구분을 잘 했는지가 궁금함

11:38
디스크레이터의 리스크매트 속이기
- 디스크 제네레이터는 디스크레이터를 이기기 위해서 만드는 것임
- 리버스 과정을 잘 거치기 위해서는 x를 ddi에서 x 제로를 예측을 해야 함
- 디스크리네이터가 엑스 제로를 잘 예측해야만 리스크매트를 속일 수 있음

15:25
신경망의 학습
- 스크림메이크가 신경 많이 학습을 해야 되는데 이미지가 많단 말임
- 얼마나 많은 걸 넣어야 참진에서 2분대가 이미지까지 오면서 망이 학습 될 수 있는지는 신기한 것 같음
- 신경망일 뿐임

18:42
ddpm과 ddim의 성능
- ddpm은 확률적 모델의 특화 패턴을 찾고 노이즈 제거에 효율적으로 되는 것임
- ddpm하고 j라고 섞었을 때 dd 라인과 어떤 성능을 낼 수 있는지가 궁금함
- gdpm이 가지고 있던 약점 부분들을 zn이라고 하는 신경망을 집어넣으면 ddim보다 더 좋은 성능을 바라지는 않을까라고 하는 논문일 수도 있을 것 같음

23:14
디스크리메이터의 역할
- 디스크리메이터는 xt 마이너스 1 프라임이 들어오면 페이크라고 판단을 내려야 함
- xt 마이너스 1은 수직적으로 원본에서 나온 거임
- 티 마이너스 1은 티에서 티 마이너스 1 전 단계 한 단계 전에 구분하는 거임
- 티 마이너스 1은 이미지에서 노이즈 낀인 건지 생성된 이미지에서 노이즈 낀인 건지 그거로만 구분할 수 있음

29:39
포스트 포스테리아 샘플링
- 포스트 포스테리아 샘플링은 확률 분포랑 확률 분포를 동시에 조건으로 해서 가는 것임
- 포스트 포스테리아 샘플링은 순수한 가오시 노이즈가 아니기 때문에 구분 가능함
- 포스트 포스테리아 샘플링은 엔비디아에서 공식적으로 발표함

34:58
디노이징
- 디노이징에 초점이 맞춰져 있어서 노이즈가 많이 끼면 이 정도로 보면 되고 노이즈가 이 정도 끼면 이 정도로 보면 됨
- 샘플링 속도가 빠르고 결과도 좋을 것 같음
- 제너레이터가 엑스 제로 프라임을 생성해낼 때 엑스 제로랑 얼마나 차이가 있는지 보통 오토인 코더 같은 경우에도 엑스제로 넣으면 원본이 나와야 되잖아요. 원본이 나올 때 그때 l2 o을 쓰거든요. x 입력으로 들어간 x 제로랑 출력으로 나오는 x 제로 프라임이랑 얼마나 차이가 있는지를 학습 시켜야 함

41:28


46:04
제너레이터의 로스
- 강사님은 제너레이터로 만든 엑스티 프라임미너 t 마이너스 1로 들어가는 이미지가 들어왔을 때 제너레이터를 학습하는 걸로 학습한다고 함
- 질문자의 질문은 제너레이터로 만든 엑스티 프라임미너 t 마이너스 1로 들어가는 이미지가 들어왔을 때 제너레이터의 로스는 어떻게 되느냐임

50:21
힐링을 할 수 있는 논문
- 힐링을 할 수 있는 두 논문이 두 가지의 모든 장점에 힐링을 할 수 있음
- fid를 측정했을 때 괜찮게 나옴
- 간에도 못하는 데이터셋을 얘가 했었던 것 같음
- 이 논문에서 나온 3개의 동그라미가 이 논문에서 나온 것임
- 이 논문에서 나온 3개를 우리는 다 어우르는 모델이라고 말을 하고 있음

55:07
축구와 춤의 차이
- 다양성을 확보하려고 축구를 할 때 인퍼런스를 할 때 어떤 다양한 이미지를 얻기 위해서 어떤 축구를 할 때 어떻게 진행이 되는지 다양한 인재가 없게 돼서 트레이닝을 할 때는 티스 1에 대한 학습이 진행이 되는데 춤을 할 때는 어떻게 진행이 되는지 질문함
- 이피전이 결국에 바이벌스티를 해결하고 패스트 샘플링은 잘 안 되는 건데 어떻게 바이벌스티가 잘 하느냐 이대리 만족하면서 다이버스를 하느냐 이거는 수식에 의해서 말을 했었다라고 기억을 함
- 디퓨전 같은 경우에는 모든 학습 데이터의 디스트리뷰션을 맞추기 하기 위해서 하는 어떤 로이즈 액실론을 퍼네이션 하고 빼는 과정이 존재를 해요. 그렇기 때문에 디스트리뷰전을 잘 대응할 수가 있어서 디스트리뷰전을 퓨전은 수학적으로 다이버스티를 하면서 피젤리들이 만족을 하는 반면에 샘플링이 좀 느리다라는 게 있는 거지 간은 좀 불안정한 모습 때문에 패스트 샘플링이랑 어떤 피델리티를 보장할 수는 있겠지만 다이버시티를 좀 보장을 할 수는 없더라라는 거임

58:23
디스크리미네이터의 오그테이션
- 디스크리미네이터를 학습할 때 노이즈를 서메이션 해가지고 오그맨테이션 하는 법도 있음
- 스타일 같은 학습할 때 데이터 셋이 많이 모자라서 보통 오그테이션이라고 하는 거는 데이터 수를 늘리는 것임
- 디스크리미네이터를 학습하는데 노이즈를 조금만 끼얹을 수도 있고 많이 끼얹을 수도 있음

1:03:09
베리에이션 오토인 코드의 의미
- 베리에이션 오토인 코드 같은 경우에는 레이턴트 벡터를 샘플링 해가지고 아웃풋을 뽑아보는 실험을 많이 해볼 수 있음
- 베리에이션 오토인 코드의 단계에 대한 의미가 각각의 의미가 있는지 없는지 그런 것들을 고려한 게 있느냐라고 했을 때 연세대학교 논문을 찾아보라고 말하는 것 같음

1:07:16
디퓨전 논문 소개
- 디퓨전 논문은 다 비슷함
- 7 단계에 걸쳐서 읽으실 수가 있음
- 디퓨전 기초를 공부하실 필요 없고 이후에 강의에서 진행되는 디퓨전 논문들에 대한 소개를 편안하게 들으실 수 있을 거라고 생각함
- 다음 주는 텍스트 이미지를 기반으로 생각을 함


clovanote.naver.com