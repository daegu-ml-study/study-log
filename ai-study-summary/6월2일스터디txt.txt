새로운 노트
2023.06.02 금 오후 1:11 ・ 74분 10초
이준택

AI 요약
00:04
ddim의 개념
- 패션이 계속해서 gdp의 ddim에 있던 개념들로부터 확장된 업무들이 굉장히 많음
- ddim은 x2를 구할 때 x1과 함께 x 제로가 주어져야 구할 수 있다는 형식으로 논리를 전개해 나감
- ddim의 코워드는 dd pm과 동일함
- ddim은 정규 분포를 따르는 어떤 거라고 보기 때문에 밍과 스탠다드 데비에이션의 형태로 해서 st 마이너스 1이 구해진다라고 하면 그렇게 정리 수식을 정확하게 다 하나하나 이해하려고 하면 그거는 더 깊게 보셔야 될 것 같음

05:39
ddpm의 리트 과정
- 정규 분포로부터 샘플링해서 xt 마이너스 1이 되어진다고 정리하면 될 것 같음
- f 3타수 티라고 fc 스티는 어떤 로이즈 엑스 t가 들어왔을 때 엑스 제로를 예측하는 방식이 다름
- x 제로에 의한 수식으로 저런 식으로 x t 마이너스 1이 됐다라는 건데 사실 결국 보면 어떻게 하면 책이 있음
- 두 가지 방식이 있는데 첫 번째는 우리 ddpm 할 때 이 수식 익숙하셔야 될 텐데 기억이 나셔야 될 텐데 이거 같은 경우에는 우리가 ddpm에서 리트 과정을 구할 데 있어서 어떤 뉴랑 트다이데베이션을 구하고 그 뉴에다가 우리 x 제로를 x t로 표현해가지고 데이 해가지고 넣었었음
- 이번에는 xt를 x 제로로 바꿔가지고 넣어보라고 함

11:28
노이즈로부터 이미지를 만들어내는 과정
- ddpm에서 노이즈로부터 스탠드 애니메이션 하는 과정으로 이미지를 만들어냄
- 노이즈로부터 이미지를 만들어내는 과정을 학습하게 되면 인퍼런스에서도 xt가 주어졌을 때 바로 x 제로를 예측할 수 있는 게 아닌가라고 생각함
- x 제로를 예측하는 건 동일함

14:42
어그멘테이션 효과
- 학습을 하다 보면 어그멘테이션 효과를 노리기 위해서 예를 들면 1번 이미지를 가지고 11번이랑 매칭을 해서 학습을 시킨다거나 50번 때쯤에서 150번 때쯤에 매칭을 시켜서 학습을 한다거나 그런 식이 가능할 것 같음
- 어그멘테이션이 효과가 거의 없을 수도 있을 것 같다는 생각이 듦
- 학습할 때는 어떤 미미의 xt를 넣어가지고 키는 랜덤하게 텐플링이 될 것임

18:06
신경망의 학습
- 신경망은 딥러닝으로 수식을 돌리면서 학습하다 보면 에프세터가 학습됨
- 신경망은 원본 이미지를 예측해 주는 신경망이라고 했는데 실제로는 100번째에서 0번째 500번째에서 0번째 이렇게 하니까 그렇게 학습할 수도 있음
- 데이터 오그멘테이션 할 수 있지 않느냐라고 묻는데 강의하시는 분이 그 질문에 대해서 잘 이해를 못하고 있는 것 같음

20:39
임플런스의 효과
- 인퍼런스 할 때는 스텝을 많이 뛰게 되니까 그만큼의 효과를 하게 되지 않을까라는 질문인 것 같은데 맞으신지 물음
- 임플런스 할 때는 스텝을 뛰는 거지 한 방에 추측할 수 없다고 말씀을 해 주시는 거임
- 스텝을 많이 뛰면 성능이 안 좋을 것 같은데 최적화 비슷하게 하다 보니까 100 단위로 뛰면 좋겠다. 이런 게 나오는 건가요 물음

25:49
신경망의 성능이 떨어지는 이유
- 신경망이 다 학습됐을 때 노이즈를 넣어도 원본 이미지가 나와야 함
- 노이즈를 예측하는 모델을 만든 것임
- gdpm 같은 경우가 성능이 안 좋아지는 이유는 스텝 티가 굉장히 크기 때문임

29:02
ddim의 백신론
- 학습이 설량이 다 됐다 한들 다 된 게 백신론을 정확하게 맞춘다는 건 아님
- ddpm의 스텝을 줄여가지고 하면은 성능이 그렇게 좋게 나오지는 않음
- ddim 논문에 있기는 한데 그거를 뺐음
- ddim에서 인퍼런스 할 때 논리가 에프 세터에 넣으면 x 제로를 예측하기 때문에 건들 수 있음
- 학습할 때는 하나씩 건너뛰니까 액실로는 작은데 인퍼런스 할 때는 100씩 건너뛰면 다름
- 학습할 때는 하나씩 건너뛰니까 상관이 없음

34:38
이미지 합성
- 원하는 문장을 한 방에 다 뽑는 게 아니고 걔들도 단어 만들어가면서 만들어가면서 앞에 문장을 생성해 가면서 문장이 쭉 생성됨
- 그림도 노이즈 같은 거 제일 틈에 딱 던져주면 거기서 원본으로 바로 튀어 나오는 게 아니고 엑스티였으니까 엑스티 마이너스 1 번째 이미지 좀 생성했다가 또 그걸로 한 번 더 돌리고 함
- 두 이미지 간에 콜레이션이나 이런 게 있을 수 있으니까 어려움

36:53
ddp의 마코프 체인
- t가 거리가 멀면 로스를 적게 환송한다든지 해볼 수 있음
- ddp는 마코프 체인을 가져오라고 함
- ddim은 체인을 끊었기 때문에 바로 이전 스텝에만 영향을 받는 게 아니고 x 제로가 필요한 x 0에 의해서 x3를 갈 수가 있음

38:47
애플, 인크리십 해서 인크레십
- 인크리십 해서 인크레십 하라는 거냐 네 그렇다라고 해석해도 상관없을 것 같음
- 심플시트하다는 게 뭔 말인지 검색해보겠었는데 임플리시틱이라네요. 뭘 지도 모르겠음
- 애플 세터 엑스틱 라이프 제로를 예측하는 게 인크리십 해서 인크레십 하라는 거냐 네 그렇다라고 해석해도 상관없을 것 같음

45:16
코어 리드 스타일
- 프로그레싱 가능한 스타일 아닌 코어한 리드랑 리드랑 화이나 스타일 이렇게 나눈 것처럼 얘네도 이제 스텝에서 디오이징 할 때 인로이징 하는 어떠한 요 각각의 요 스텝을 세 부분으로 나누고 세 부분이 프로스트 니들 파일 정보들을 배우게 됨
- 네트워크에서 이런 식으로 아마 냈던 걸로 기억을 하는데 만약에 관심이 있다면 edb라는 어떤 mbd아의 논문을 한번 읽어보시면 그런 것들이랑은 좀 다를 것임
- 학습을 통해서 인무징 하는 걸 학습을 하는 것이기 때문에 성능에 큰 영향이 있을지는 모르겠음

51:11
디퓨전 모델의 활용
- ddi형 gdpm만 포워드랑 노스는 동일함
- ddtm이랑 ddim에 의해서 디퓨전이 어떤 식으로 디퓨션의 뭔지와 그리고 디디 프레임과 ddi를 해서는 그 디퓨션을 어떻게 네트워크에 학습을 해서 노스를 개선하고 인퍼런스 했는지를 배움
- 디퓨전 모델이 반을 이기는 논문으로 유니스 20 1에 나왔던 논문임
- 가이드 디디퓨전이라고 해서 디퓨전을 어떤 컨디셔널 제너레이션에 활용을 함
- 어댑티브 그룹 노말리제이션을 활용한 것이 세 가지를 기억하면 됨

56:25
노마리제이션의 개념
- 노마리제이션을 인스턴스 놈을 활용하면 에이다 인, 노마리제이션을 그룹 노마리제이션을 활용하면 어넥티브라고 말함
- 아빠의 노말리게이션은 당연히 알겠지만 간단하게 표현하면 그런 놈에 있는 페이퍼를 그냥 가져온 것임
- 배치 단위로 배치하는 것을 배치노라고 함
- 스타일만 계열의 유산이 왜 이후 생산 모델로 이어지지 않을지에 대해 물음

58:53
인스턴스 노멀리제이션
- 인스턴스 노멀리제이션은 아다인에 대한 설명임
- 인스턴스 노멀리제이션은 스타일에 관련된 충만 쫙 뽑아가지고 노멀레이제이션 해서 layer로 넣어서 함
- 스타일을 mrlzation 하는 방식으로 레이어마다 스타일을 가지게끔 normalization 해서 layer로 넣어서 함

1:01:34
트럼페이션 트릭
- 트럼페이션 트릭을 다시 한 번 랩업을 하려고 가져옴
- 인퍼런스를 할 때 어떤 스탠바이데이션을 잘라가지고 끝단에 주어가는 값들을 다 잘라가지고 프럼 페이지에서 아웃시한 로마게게이션을 인풋으로 넣어서 하는 거임
- 얼마나 자르느냐에 따라서 피델리티가 높아지는 대신 다이버스티가 적거나 수리 치가 낮아지는 대신 바이버트가 높아지거나 혈액을 조절할 수 있음
- fid를 계산할 수 있는 매트임

1:03:59
디노이징 된 이미지의 분포
- 어떤 레이블과 함께 어떤 노이지 인 xt 플러스 1이 들어왔을 때 디노이징 된 xt 같은 경우에는 다음과 같은 식의 분포로 표현이 될 수 있음
- 어떤 레이블을 구했을 때 그 분포와 엑스 티 플러스 노이즈 낀 이미지에서 디모이징 된 엑스티 이 두 분포 간의 어떤 고부로 얘를 표현할 수 있음
- 제트는 옆에 있는 두 분포를 pdf 프로티 댄스티 펑션으로 만들어주기 위한 상수라고 보시면 되고 굳이 크게 고민하실 필요는 없음

1:09:01
생산 모델의 조건
- 생성 모델을 만들 때 학습 이미지로 사용했던 것들만 만들어지면 진정한 생산 모델이 아님
- 학습 데이터에 없었던 걸 새로운 걸 만들어내줘야 좋은 성능 좋은 생산 모델임
- 다이버스티가 높아지기 위해서 트릭 기법들을 쓰는 것 중에 하나가 클래식 파이어 가이던스 주는 것 같음

1:10:17
라벨의 중요성
- 라벨이 주어졌으니까 수식 전개가 됐다고 이해하면 될 것 같음
- 라벨 쪽으로 너무 치우치게 되면 원본 이미지랑 디스턴스가 있으니까 퀄리티가 떨어지고 그런 설명을 한 거임


clovanote.naver.com